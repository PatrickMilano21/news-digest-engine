# Architecture Blueprint (High-Level, Reusable)

## Purpose
This is a reusable blueprint for AI-backed services. It is intentionally high-level so it can be quickly adapted per project.
It is a guide, not a law. Deviate with explicit rationale.

Goals:
- correctness and debuggability
- clear boundaries and ownership
- long-term velocity in AI-heavy systems
- safe use of AI tooling

---

## Explicit Non-Goals
This blueprint does NOT attempt to:
- Choose specific frameworks or vendors
- Define frontend architecture
- Optimize for premature scale
- Replace project-specific design docs

---

## When This Blueprint Is Overkill
This blueprint may be excessive for:
- One-off analysis scripts
- Internal notebooks with no users
- Exploratory research without persistence

---

## Core Principles
- Boundaries beat abstractions: keep responsibilities explicit.
- Determinism where it matters: make behavior testable and reproducible.
- Contracts over vibes: schemas, refusals, and traceability are explicit.
- Observability is part of the product, not an afterthought.
- Customer trust is a first-class requirement.

---

## Layering Model (Conceptual)
Transport → Presentation → Domain → Persistence → External Dependencies → Batch Orchestration → Evals/Observability

The filenames may change; the boundaries should not.

---

## Canonical Repo Structure (Reusable)
This is a default layout you can adapt per project.

src/
- main.py (transport only)
- views.py (presentation/view models)
- domain/ (pure business logic)
- repo.py + db.py (persistence)
- clients/ (external APIs, LLMs)
- llm_schemas/ (structured outputs)
- artifacts.py (static outputs)
- evals/ (quality checks)
- logging/errors (observability)

templates/ (customer UI)
jobs/ (batch orchestration)
tests/ (mirrors src/)
artifacts/ (generated outputs, gitignored)
data/ (local state, gitignored)

Isolation rules (good practice)
- No raw SQL outside repo layer
- No business logic in routes
- No debug data in customer UI
- No runtime agents controlling production flow without eval-gated safeguards

---

## Surfaces (Always Separate)
- Customer surface: trust-first, minimal, no debug data
- Operator surface: full diagnostics, audits, and failure context
- Artifacts: durable, inspectable outputs (for audits, email, reports)

---

## AI Autonomy (Evidence-Gated)
AI autonomy increases only when reliability is proven by evals.
Autonomy expands in stages:
- Assist → Supervised → Gated Autonomy → Trusted Automation

If evals degrade, autonomy rolls back.

---

## Change Lifecycle (Critical)
All new AI behavior (agents, prompts, heuristics, automation) moves through stages:

1) Proposal
   - Idea generated by human or AI
   - No production impact
   - Documented intent + risk

2) Sandbox
   - Runs on fixtures, logs, or shadow data
   - No customer-facing effects
   - Eval expectations defined

3) Gated Integration
   - Behind feature flags or scopes
   - Evals must pass
   - Operator-visible only

4) Promotion
   - Enabled for real workflows
   - Observability + rollback required

Nothing skips stages. Fast is fine. Skipping is not.

---

## Evals (Conceptual Model)
Evals answer three questions:
1) Regression: Did something break?
2) Capability: Can the system do what we expect?
3) Safety/Trust: Did it violate a rule?

Typical eval forms:
- Deterministic tests (golden cases)
- Heuristic checks (coverage, completeness)
- Invariant checks (must-never-happen)

Evals gate autonomy, promotion, and release.
If an eval fails, behavior rolls back.

---

## Eval Types (High-Level)
1) Invariant evals (must-never-happen)
   - Safety violations
   - Contract breaks
   - Hallucinations
   - Refusal failures

Invariant failures always block promotion.

---

## Failure Taxonomy (Learning Asset)
Failures are classified, counted, and reviewed.
Patterns drive new evals and design changes.
Unclassified failures are system debt.

---

## Eval Cost Discipline
Evals should be cheap enough to run continuously.
If they are too slow to run, they are too slow to trust.

---

## Sandbox & Exploration (Required)
Every project must have a non-production sandbox where:
- AI can propose new agents, heuristics, or prompts
- Experiments run on logs, fixtures, or snapshots
- Results are visible only to operators

The sandbox is where creativity lives.
Production is where trust lives.

---

## Agent Roles (Conceptual)
Common agent roles include:
- Builder: drafts code, prompts, schemas
- Reviewer: checks against architecture, safety, evals
- Red-Teamer: searches for failures or abuse
- Analyst: proposes improvements based on data

Agents may assist creation. Agents may not self-promote to production.

---

## Agent Scope & Tool Contracts
Agents must have:
- Explicit task scope
- Explicit allowed tools
- Explicit forbidden actions

Undefined scope is a failure.

---

## Human-in-the-Loop (Default)
Early-stage agents require:
- Review before promotion
- Operator visibility into decisions
- Easy rollback or override

Removing human review requires evidence.

---

## Agents Propose, Systems Decide
Agents may propose:
- Code changes
- New evals
- New workflows
- Automation opportunities

Only the system (via gates) executes.

---

## System Diagram (Optional)
Include a single high-level diagram showing:
- Data flow
- AI invocation points
- Eval checkpoints

---

## Review & Learning Topics (Keep These High-Level)
These are the universal topics to check in every project:
1) Architecture alignment
2) AI leverage (tools, agents, automation opportunities)
3) Evals/tests (coverage and regressions)
4) Trust boundaries (grounding, refusals, user clarity)
5) Observability (run visibility, failure taxonomy)
6) Security/privacy
7) Complexity reduction
8) Performance/cost
9) Surface separation
10) Release readiness

---

## Continuous Learning & Suggestions
The system should regularly surface:
- Failed eval patterns
- User friction signals
- Cost or latency anomalies
- Opportunities for automation or simplification

Suggestions may come from:
- Humans
- Agents
- Usage data

Suggestions are inputs, not actions. They enter the Change Lifecycle.

---

## Reference Guides (When Designing Agents/Evals)
- OpenAI agent guide: https://openai.com/business/guides-and-resources/a-practical-guide-to-building-ai-agents/
- Anthropic evals guide: https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents

---

## Project Blueprint Template (Copy Per Repo)
Use this to create a project-specific blueprint in minutes.

1) System summary
- What the system does
- Who the user is
- What “success” means

2) Surfaces
- Customer UI: must show / must hide
- Operator UI: required diagnostics
- Artifacts: what must be captured

3) Trust boundaries
- What must be grounded
- When to refuse
- What is non-negotiable

4) Evals & tests
- Core evals (regression + capability)
- Project-specific evals
- Test gates

5) AI leverage plan
- Where agents help (and where they do not)
- Tooling/mcp/skills/hook candidates

6) Failure handling
- Failure taxonomy
- What gets logged
- How users are informed

7) Data & storage
- What is persisted
- What is cacheable
- What is ephemeral

8) Release readiness
- Runbook
- Monitoring
- Rollback plan

---

## Final Principle
If a user complains, an eval fails, or an AI behaves unexpectedly, the system should be explainable without reading code.
That's the value of this blueprint.
